{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __MODEL NOTEBOOK__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *press these links to see the training notebooks*\n",
    "[Model for detecting fall using CNN (Image classification)](https://www.kaggle.com/code/sahiltarlana2601/ai-sis-fall/edit)\n",
    "\n",
    "[Model for detecting fall using Random forest with sensor data](https://www.kaggle.com/code/sahiltarlana2601/big-fall-pranav)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __IMPORTING REQUIRED MODULES__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.patches as patches\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "from twilio.rest import Client\n",
    "import keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __VERIFYING VERSION OF TENSOR FLOW__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __LOADING MODEL DOWNLOADED FROM KAGGLE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model(\"pranav_sahil_final.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __LOADING YOLOV8 MODEL__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Set the input size using the model's overrides parameter\n",
    "yolo_model.model.args[\"imgsz\"] = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes_vid(frame):\n",
    "    img = frame\n",
    "    image_width, image_height, _ = img.shape\n",
    "    results = yolo_model(img)\n",
    "    myboxes = []\n",
    "    for result in results:\n",
    "        boxes =result.boxes\n",
    "        for box in boxes:\n",
    "            class_id=box.cls\n",
    "            confidence=box.conf.item()\n",
    "            x_min, y_min, x_max, y_max = box.xyxy[0]\n",
    "            if class_id==0 and  confidence > 0.3:\n",
    "                # print(f\"Bounding box coordinates: ({x_min:.2f}, {y_min:.2f}), ({x_max:.2f}, {y_max:.2f})\")\n",
    "                temp = [x_min, y_min, x_max, y_max]\n",
    "                myboxes.append(temp)\n",
    "    return myboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame=cv2.imread(\"test_images/sample5.jpg\")\n",
    "# get_boxes_vid(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_size = (128, 128)\n",
    "def pred_vid(frame):\n",
    "    list_fall=[]\n",
    "    bounding_boxes=get_boxes_vid(frame)\n",
    "    img=frame\n",
    "    complete_images=[]\n",
    "    if(bounding_boxes==None):\n",
    "            return list_fall\n",
    "    for box in bounding_boxes:\n",
    "        image_height, image_width, _ = img.shape\n",
    "        x_min, y_min, x_max, y_max = box[:]\n",
    "        print(f\"Bounding box coordinates: ({x_min:.2f}, {y_min:.2f}), ({x_max:.2f}, {y_max:.2f})\")\n",
    "        complete_images.append(img[int(y_min):int(y_max), int(x_min):int(x_max)])\n",
    "    print(len(complete_images))\n",
    "    for cropped_img in complete_images:\n",
    "        print(1234)\n",
    "        cropped_img_resized = cv2.resize(cropped_img, pref_size)\n",
    "        # plt.imshow(cropped_img_resized)\n",
    "        # plt.show()\n",
    "        cropped_img_resized = cropped_img_resized / 255.0\n",
    "        # print(cropped_img_resized.shape)\n",
    "        cropped_img_resized = np.expand_dims(cropped_img_resized, axis=0)\n",
    "        predictions = model.predict(cropped_img_resized)\n",
    "        print(predictions)\n",
    "        if(predictions[0][0]>=0.55):\n",
    "            list_fall.append(1)\n",
    "        else:\n",
    "            list_fall.append(0)\n",
    "    return list_fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame=cv2.imread(\"test_images/sample5.jpg\")\n",
    "# plt.imshow(frame)\n",
    "# plt.show()\n",
    "# values=pred_vid(frame)\n",
    "# if (len(values)==0):\n",
    "#             print(\"no person detected\")\n",
    "# else:\n",
    "#     for val in values:\n",
    "#         if val == 1:\n",
    "#             print(\"fall detected\")\n",
    "#             client=Client(keys.account_sid,keys.auth_token)\n",
    "#             message=client.messages.create(\n",
    "#             body=\"Fall Detected\",\n",
    "#             from_=keys.twilio_number,\n",
    "#             to=keys.my_phone_number\n",
    "#             )\n",
    "#         elif val == 0:\n",
    "#             print(\"no fall detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame=cv2.imread(\"test-yolo.jpg\")\n",
    "\n",
    "# pred_vid(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 chair, 74.9ms\n",
      "Speed: 0.0ms preprocess, 74.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Bounding box coordinates: (216.00, 99.51), (329.99, 412.17)\n",
      "1\n",
      "1234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "[[ 1.5383e-07]]\n",
      "no fall detected\n",
      "\n",
      "0: 480x640 1 dog, 1 chair, 1 bed, 84.6ms\n",
      "Speed: 0.0ms preprocess, 84.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "0\n",
      "no person detected\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 1 bed, 1 laptop, 84.6ms\n",
      "Speed: 0.0ms preprocess, 84.6ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Bounding box coordinates: (0.00, 0.00), (238.44, 278.55)\n",
      "Bounding box coordinates: (0.07, 0.09), (243.64, 470.11)\n",
      "2\n",
      "1234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "[[   0.015158]]\n",
      "1234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "[[  1.868e-06]]\n",
      "no fall detected\n",
      "no fall detected\n",
      "\n",
      "0: 480x640 1 person, 1 backpack, 1 book, 90.3ms\n",
      "Speed: 0.0ms preprocess, 90.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Bounding box coordinates: (87.15, 104.91), (334.81, 389.95)\n",
      "1\n",
      "1234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "[[ 0.00065036]]\n",
      "no fall detected\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 1 bed, 1 book, 87.2ms\n",
      "Speed: 0.0ms preprocess, 87.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Bounding box coordinates: (71.04, 77.95), (312.92, 373.70)\n",
      "1\n",
      "1234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "[[ 0.00030434]]\n",
      "no fall detected\n",
      "\n",
      "0: 480x640 1 person, 1 cup, 1 chair, 75.3ms\n",
      "Speed: 1.5ms preprocess, 75.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Bounding box coordinates: (47.85, 55.95), (293.83, 363.81)\n",
      "1\n",
      "1234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "[[   0.002754]]\n",
      "no fall detected\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"http://192.168.108.124:4747/video\")\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "    frame = cv2.resize(frame, None, fx=1, fy=1, interpolation=cv2.INTER_AREA)\n",
    "    window_width = 600  \n",
    "    window_height = 600\n",
    "    cv2.namedWindow('Webcam', cv2.WINDOW_NORMAL)  \n",
    "    cv2.resizeWindow('Webcam', window_width, window_height)\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('p'):\n",
    "        values=pred_vid(frame)\n",
    "        if (len(values)==0):\n",
    "            print(\"no person detected\")\n",
    "        else:\n",
    "            for val in values:\n",
    "                if val == 1:\n",
    "                    print(\"fall detected\")\n",
    "                    client=Client(keys.account_sid,keys.auth_token)\n",
    "                    message=client.messages.create(\n",
    "                    body=\"Fall Detected\",\n",
    "                    from_=keys.twilio_number,\n",
    "                    to=keys.my_phone_number\n",
    "                    )\n",
    "                elif val == 0:\n",
    "                    print(\"no fall detected\")\n",
    "            \n",
    "    elif key == ord('q'): \n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
